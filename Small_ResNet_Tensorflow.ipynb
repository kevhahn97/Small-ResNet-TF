{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Small-ResNet-Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTCIo3o7E7M1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets.cifar10 import load_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RNtfJgpPQ26",
        "colab_type": "code",
        "outputId": "56afa08e-fa47-4755-cbca-ebab3a042e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "(trainX, trainY), (testX, testY) = load_data()\n",
        "trainY = np.eye(10)[trainY].reshape(trainY.shape[0],10)\n",
        "testY = np.eye(10)[testY].reshape(testY.shape[0],10)\n",
        "\n",
        "print('Data loaded')\n",
        "print('Train:', len(trainX), 'Test:', len(testX))\n",
        "print('Shapes:', trainX.shape, trainY.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data loaded\n",
            "Train: 50000 Test: 10000\n",
            "Shapes: (50000, 32, 32, 3) (50000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DfwON4sPT3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "epsilon = 1e-3\n",
        "Input = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# Residual Block 1\n",
        "W1 = tf.Variable(tf.random_normal([3, 3, 3, 16], stddev=0.01))\n",
        "Conv_1 = tf.nn.conv2d(Input, filter=W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "batch_mean1, batch_var1 = tf.nn.moments(Conv_1,[0])\n",
        "L1_hat = (Conv_1 - batch_mean1)/tf.sqrt(batch_var1+epsilon)\n",
        "scale1 = tf.Variable(tf.ones([16]))\n",
        "beta1 = tf.Variable(tf.ones([16]))\n",
        "BN_1 = scale1*L1_hat + beta1\n",
        "#BN_1 = tf.layers.batch_normalization(Conv_1)\n",
        "Activation_1 = tf.nn.relu(BN_1)\n",
        "W2 = tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=0.01))\n",
        "Conv_2 = tf.nn.conv2d(Activation_1, filter=W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "batch_mean2, batch_var2 = tf.nn.moments(Conv_2,[0])\n",
        "L2_hat = (Conv_2 - batch_mean2)/tf.sqrt(batch_var2+epsilon)\n",
        "scale2 = tf.Variable(tf.ones([16]))\n",
        "beta2 = tf.Variable(tf.ones([16]))\n",
        "BN_2 = scale2*L2_hat + beta2\n",
        "#BN_2 = tf.layers.batch_normalization(Conv_2)\n",
        "Activation_2 = tf.nn.relu(BN_2)\n",
        "W3 = tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=0.01))\n",
        "Conv_3 = tf.nn.conv2d(Activation_2, filter=W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "batch_mean3, batch_var3 = tf.nn.moments(Conv_3,[0])\n",
        "L3_hat = (Conv_3 - batch_mean3)/tf.sqrt(batch_var3+epsilon)\n",
        "scale3 = tf.Variable(tf.ones([16]))\n",
        "beta3 = tf.Variable(tf.ones([16]))\n",
        "BN_3 = scale3*L3_hat + beta3\n",
        "#BN_3 = tf.layers.batch_normalization(Conv_3)\n",
        "Add_1 = tf.add(Activation_1, BN_3)\n",
        "Activation_3 = tf.nn.relu(Add_1)\n",
        "Max_Pool_1 = tf.nn.max_pool(Activation_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# Residual Block 2\n",
        "W4 = tf.Variable(tf.random_normal([3, 3, 16, 32], stddev=0.01))\n",
        "Conv_4 = tf.nn.conv2d(Max_Pool_1, filter=W4, strides=[1, 1, 1, 1], padding='SAME')\n",
        "batch_mean4, batch_var4 = tf.nn.moments(Conv_4,[0])\n",
        "L4_hat = (Conv_4 - batch_mean4)/tf.sqrt(batch_var4+epsilon)\n",
        "scale4 = tf.Variable(tf.ones([32]))\n",
        "beta4 = tf.Variable(tf.ones([32]))\n",
        "BN_4 = scale4*L4_hat + beta4\n",
        "#BN_4 = tf.layers.batch_normalization(Conv_4)\n",
        "Activation_4 = tf.nn.relu(BN_4)\n",
        "W5 = tf.Variable(tf.random_normal([3, 3, 32, 32], stddev=0.01))\n",
        "Conv_5 = tf.nn.conv2d(Activation_4, filter=W5, strides=[1, 1, 1, 1], padding='SAME')\n",
        "batch_mean5, batch_var5 = tf.nn.moments(Conv_5,[0])\n",
        "L5_hat = (Conv_5 - batch_mean5)/tf.sqrt(batch_var5+epsilon)\n",
        "scale5 = tf.Variable(tf.ones([32]))\n",
        "beta5 = tf.Variable(tf.ones([32]))\n",
        "BN_5 = scale5*L5_hat + beta5\n",
        "#BN_5 = tf.layers.batch_normalization(Conv_5)\n",
        "Activation_5 = tf.nn.relu(BN_5)\n",
        "W6 = tf.Variable(tf.random_normal([3, 3, 32, 32], stddev=0.01))\n",
        "Conv_6 = tf.nn.conv2d(Activation_5, filter=W6, strides=[1, 1, 1, 1], padding='SAME')\n",
        "batch_mean6, batch_var6 = tf.nn.moments(Conv_6,[0])\n",
        "L6_hat = (Conv_6 - batch_mean6)/tf.sqrt(batch_var6+epsilon)\n",
        "scale6 = tf.Variable(tf.ones([32]))\n",
        "beta6 = tf.Variable(tf.ones([32]))\n",
        "BN_6 = scale6*L6_hat + beta6\n",
        "#BN_6 = tf.layers.batch_normalization(Conv_6)\n",
        "Add_2 = tf.add(Activation_4, BN_6)\n",
        "Activation_6 = tf.nn.relu(Add_2)\n",
        "Max_Pool_2 = tf.nn.max_pool(Activation_6, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# Residual Block 3\n",
        "W7 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "Conv_7 = tf.nn.conv2d(Max_Pool_2, filter=W7, strides=[1, 1, 1, 1], padding='SAME')\n",
        "batch_mean7, batch_var7 = tf.nn.moments(Conv_7,[0])\n",
        "L7_hat = (Conv_7 - batch_mean7)/tf.sqrt(batch_var7+epsilon)\n",
        "scale7 = tf.Variable(tf.ones([64]))\n",
        "beta7 = tf.Variable(tf.ones([64]))\n",
        "BN_7 = scale7*L7_hat + beta7\n",
        "#BN_7 = tf.layers.batch_normalization(Conv_7)\n",
        "Activation_7 = tf.nn.relu(BN_7)\n",
        "W8 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n",
        "Conv_8 = tf.nn.conv2d(Activation_7, filter=W8, strides=[1, 1, 1, 1], padding='SAME')\n",
        "batch_mean8, batch_var8 = tf.nn.moments(Conv_8,[0])\n",
        "L8_hat = (Conv_8 - batch_mean8)/tf.sqrt(batch_var8+epsilon)\n",
        "scale8 = tf.Variable(tf.ones([64]))\n",
        "beta8 = tf.Variable(tf.ones([64]))\n",
        "BN_8 = scale8*L8_hat + beta8\n",
        "#BN_8 = tf.layers.batch_normalization(Conv_8)\n",
        "Activation_8 = tf.nn.relu(BN_8)\n",
        "W9 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n",
        "Conv_9 = tf.nn.conv2d(Activation_8, filter=W9, strides=[1, 1, 1, 1], padding='SAME')\n",
        "batch_mean9, batch_var9 = tf.nn.moments(Conv_9,[0])\n",
        "L9_hat = (Conv_9 - batch_mean9)/tf.sqrt(batch_var9+epsilon)\n",
        "scale9 = tf.Variable(tf.ones([64]))\n",
        "beta9 = tf.Variable(tf.ones([64]))\n",
        "BN_9 = scale9*L9_hat + beta9\n",
        "#BN_9 = tf.layers.batch_normalization(Conv_9)\n",
        "Add_3 = tf.add(Activation_7, BN_9)\n",
        "Activation_9 = tf.nn.relu(Add_3)\n",
        "Max_Pool_3 = tf.nn.max_pool(Activation_9, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "Avg_Pool = tf.nn.avg_pool(Max_Pool_3, ksize=[1,8,8,1], strides=[1,8,8,1], padding='SAME')\n",
        "\n",
        "# Dense layer\n",
        "Flatten = tf.reshape(Avg_Pool, [-1, 64])\n",
        "W10 = tf.Variable(tf.random_normal([64, 10], stddev=0.01))\n",
        "B10 = tf.Variable(tf.random_normal(shape=[10], stddev=0.01))\n",
        "model = tf.nn.softmax(tf.matmul(Flatten, W10)+B10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsTDAzMSPWug",
        "colab_type": "code",
        "outputId": "ce92fb03-b6df-4a52-b05b-d14c69bbbc6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "Lr = 0.001\n",
        "Epochs = 20\n",
        "\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(tf.clip_by_value(model, 1e-10, 1.0)), [1]))\n",
        "optimizer = tf.train.AdamOptimizer(Lr).minimize(cost)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "for epoch in range(Epochs):\n",
        "    total_cost = 0\n",
        "    total_batch = int(len(trainX)/batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = trainX[i*batch_size:(i+1)*batch_size], trainY[i*batch_size:(i+1)*batch_size]\n",
        "        _, cost_val = sess.run([optimizer, cost], feed_dict={Input: batch_xs, Y: batch_ys})\n",
        "        total_cost += cost_val\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'Avg. cost =', '{:.3f}'.format(total_cost / total_batch), 'Test Acc. = ', sess.run(accuracy, feed_dict={Input: testX, Y: testY}))\n",
        "print('Training Done!')\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 Avg. cost = 1.547 Test Acc. =  0.5432\n",
            "Epoch: 0002 Avg. cost = 1.170 Test Acc. =  0.604\n",
            "Epoch: 0003 Avg. cost = 1.024 Test Acc. =  0.6388\n",
            "Epoch: 0004 Avg. cost = 0.930 Test Acc. =  0.6718\n",
            "Epoch: 0005 Avg. cost = 0.856 Test Acc. =  0.6982\n",
            "Epoch: 0006 Avg. cost = 0.795 Test Acc. =  0.714\n",
            "Epoch: 0007 Avg. cost = 0.743 Test Acc. =  0.7308\n",
            "Epoch: 0008 Avg. cost = 0.697 Test Acc. =  0.7423\n",
            "Epoch: 0009 Avg. cost = 0.654 Test Acc. =  0.7479\n",
            "Epoch: 0010 Avg. cost = 0.616 Test Acc. =  0.7548\n",
            "Epoch: 0011 Avg. cost = 0.579 Test Acc. =  0.7599\n",
            "Epoch: 0012 Avg. cost = 0.547 Test Acc. =  0.7643\n",
            "Epoch: 0013 Avg. cost = 0.516 Test Acc. =  0.7668\n",
            "Epoch: 0014 Avg. cost = 0.488 Test Acc. =  0.7699\n",
            "Epoch: 0015 Avg. cost = 0.462 Test Acc. =  0.7718\n",
            "Epoch: 0016 Avg. cost = 0.438 Test Acc. =  0.772\n",
            "Epoch: 0017 Avg. cost = 0.414 Test Acc. =  0.7719\n",
            "Epoch: 0018 Avg. cost = 0.387 Test Acc. =  0.7733\n",
            "Epoch: 0019 Avg. cost = 0.363 Test Acc. =  0.7731\n",
            "Epoch: 0020 Avg. cost = 0.339 Test Acc. =  0.7746\n",
            "Epoch: 0021 Avg. cost = 0.319 Test Acc. =  0.7721\n",
            "Epoch: 0022 Avg. cost = 0.301 Test Acc. =  0.7721\n",
            "Epoch: 0023 Avg. cost = 0.281 Test Acc. =  0.7676\n",
            "Epoch: 0024 Avg. cost = 0.263 Test Acc. =  0.7625\n",
            "Epoch: 0025 Avg. cost = 0.250 Test Acc. =  0.7625\n",
            "Epoch: 0026 Avg. cost = 0.240 Test Acc. =  0.764\n",
            "Epoch: 0027 Avg. cost = 0.224 Test Acc. =  0.7608\n",
            "Epoch: 0028 Avg. cost = 0.207 Test Acc. =  0.7606\n",
            "Epoch: 0029 Avg. cost = 0.191 Test Acc. =  0.7613\n",
            "Epoch: 0030 Avg. cost = 0.179 Test Acc. =  0.7601\n",
            "Epoch: 0031 Avg. cost = 0.170 Test Acc. =  0.7618\n",
            "Epoch: 0032 Avg. cost = 0.161 Test Acc. =  0.7647\n",
            "Epoch: 0033 Avg. cost = 0.152 Test Acc. =  0.7632\n",
            "Epoch: 0034 Avg. cost = 0.142 Test Acc. =  0.7602\n",
            "Epoch: 0035 Avg. cost = 0.129 Test Acc. =  0.7581\n",
            "Epoch: 0036 Avg. cost = 0.125 Test Acc. =  0.7645\n",
            "Epoch: 0037 Avg. cost = 0.117 Test Acc. =  0.7658\n",
            "Epoch: 0038 Avg. cost = 0.109 Test Acc. =  0.7702\n",
            "Epoch: 0039 Avg. cost = 0.099 Test Acc. =  0.7689\n",
            "Epoch: 0040 Avg. cost = 0.092 Test Acc. =  0.7702\n",
            "Epoch: 0041 Avg. cost = 0.084 Test Acc. =  0.7698\n",
            "Epoch: 0042 Avg. cost = 0.086 Test Acc. =  0.7675\n",
            "Epoch: 0043 Avg. cost = 0.083 Test Acc. =  0.7675\n",
            "Epoch: 0044 Avg. cost = 0.078 Test Acc. =  0.7672\n",
            "Epoch: 0045 Avg. cost = 0.078 Test Acc. =  0.7633\n",
            "Epoch: 0046 Avg. cost = 0.078 Test Acc. =  0.764\n",
            "Epoch: 0047 Avg. cost = 0.080 Test Acc. =  0.7586\n",
            "Epoch: 0048 Avg. cost = 0.075 Test Acc. =  0.7635\n",
            "Epoch: 0049 Avg. cost = 0.065 Test Acc. =  0.7659\n",
            "Epoch: 0050 Avg. cost = 0.061 Test Acc. =  0.7648\n",
            "Training Done!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}